{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 4820\n",
    "# Assignment 7: Understanding how back-propagation works in a Deep Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In assignment 3, in order to understand the feed-forward process in a Deep Neural Net and what is exactly happening when the function `model.predict()` is called, we manually calculated outputs from all neurons layer by layer.\n",
    "\n",
    "This assignment is a continuation of a3, whose goal is to demystify the function `model.fit()`, i.e., break down how a Neural Net learns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Selective common activation functions and their derivatives (completed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Either use the built-in function in Keras\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "\n",
    "# Or use our own\n",
    "# def sigmoid(x):\n",
    "#    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def derivative_sigmoid(y):\n",
    "    return y * (1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Hyperbolic Tangent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.activations import tanh\n",
    "\n",
    "def derivative_tanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.activations import relu\n",
    "\n",
    "def derivative_relu(y):\n",
    "    return np.maximum(0,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Softmax and Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "\n",
    "def derivative_softmax_ce_loss(y_hat, y):\n",
    "    return y_hat - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Manual Calculations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Manually calculate the outputs from all neurons layer by layer (completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[0.9 1.2 0.7 0.8]] \n",
      "\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "X_train = np.array([[0.9,1.2,0.7,0.8]]) \n",
    "print('Inputs:\\n', X_train, '\\n')\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights between input and hidden layer 1:\n",
      " [[ 1  0  0]\n",
      " [ 0  1 -1]\n",
      " [-1  0  0]\n",
      " [ 0 -1  1]]\n",
      "Thetas in hidden layer 1:\n",
      " [0.1 0.1 0.1]\n",
      "Outputs from hidden layer 1:\n",
      " [[ 0.29131261  0.46211716 -0.29131261]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a=np.array([[1,0,-1,0]]).T\n",
    "weights_0 = np.concatenate(\n",
    "    (\n",
    "        a,\n",
    "        np.roll(a,1,axis=0),\n",
    "        np.roll(a,3,axis=0)\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "thetas_0 = np.full((3,),0.1)\n",
    "print('Weights between input and hidden layer 1:\\n', weights_0)\n",
    "print('Thetas in hidden layer 1:\\n', thetas_0)\n",
    "\n",
    "y_1=tanh(X_train.dot(weights_0)+thetas_0)\n",
    "print('Outputs from hidden layer 1:\\n', y_1.numpy(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights between hidden layer 1 and hidden layer 2:\n",
      " [[-1.  -0.8  1.3]\n",
      " [ 1.3 -1.  -0.8]\n",
      " [-0.8  1.3 -1. ]]\n",
      "Thetas in hidden layer 2:\n",
      " [0.1 0.1 0.1]\n",
      "Outputs from hidden layer 2:\n",
      " [[ 0.56659243 -0.7504016   0.38022725]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "b=np.array([[-1], [1.3], [-0.8]])\n",
    "weights_1 = np.concatenate(\n",
    "    (\n",
    "        b,\n",
    "        np.roll(b,1,axis=0),\n",
    "        np.roll(b,2,axis=0)\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "thetas_1 = np.full((3,),0.1)\n",
    "print('Weights between hidden layer 1 and hidden layer 2:\\n', weights_1)\n",
    "print('Thetas in hidden layer 2:\\n', thetas_1)\n",
    "\n",
    "y_2=tanh(y_1.numpy().dot(weights_1)+thetas_1)\n",
    "print('Outputs from hidden layer 2:\\n', y_2.numpy(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights between hidden layer 2 and output layer:\n",
      " [[-1.   1.1  0.  -1. ]\n",
      " [ 0.  -1.   1.1  0. ]\n",
      " [ 1.1  0.  -1.   1.1]]\n",
      "Thetas in output layer:\n",
      " [0.1 0.1 0.1 0.1]\n",
      "Outputs from output layer:\n",
      " [[0.14432633 0.66121078 0.05013656 0.14432633]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "c=np.array([[-1], [0], [1.1]])\n",
    "weights_2 = np.concatenate(\n",
    "    (\n",
    "        c,\n",
    "        np.roll(c,1,axis=0),\n",
    "        np.roll(c,2,axis=0),\n",
    "        np.roll(c,3,axis=0)\n",
    "    ), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "thetas_2 = np.full((4,),0.1)\n",
    "print('Weights between hidden layer 2 and output layer:\\n', weights_2)\n",
    "print('Thetas in output layer:\\n', thetas_2)\n",
    "\n",
    "# output=Activation('softmax')(y_2.numpy().dot(weights_2)+thetas_2).numpy()\n",
    "\n",
    "y_hat=softmax(tf.convert_to_tensor((np.dot(y_2, weights_2)+thetas_2))).numpy()\n",
    "print('Outputs from output layer:\\n', y_hat, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Manually calculate the cross entroy loss (completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([[0, 1, 0, 0]])  # feel free to change these values\n",
    "loss = categorical_crossentropy(y_train, y_hat)\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Manually calculate the new weights and thetas as the loss/error gradient back-propagates through the layers <span style=\"color:red\">(to be completed)</span>\n",
    "\n",
    "__IMPORTANT__: In this section, you must perform all the calculations in the form of __numpy matrix operations__, or no point will be assigned.\n",
    "\n",
    "Note here that instead of actually updating the weights and thetas after we've figured out all the deltas (i.e. the amount of changes that we would like to apply to these weights and thetas), we simply store them in a set of separate variables because we still need the original values of them in Sections 2 and 3 below for validation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14432633 -0.33878922  0.05013656  0.14432633]]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1 # learning rate\n",
    "\n",
    "error_gradient_output_layer = derivative_softmax_ce_loss(y_hat, y_train)\n",
    "\n",
    "print(error_gradient_output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 _Weights_ between hidden layer 2 and output layer as well as _Thetas_ in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00817742  1.11919554 -0.0028407  -1.00817742]\n",
      " [ 0.01083027 -1.0254228   1.10376226  0.01083027]\n",
      " [ 1.09451232  0.01288169 -1.00190633  1.09451232]]\n"
     ]
    }
   ],
   "source": [
    "delta_weights_output_layer = error_gradient_output_layer.T.dot(y_2).T * lr\n",
    "\n",
    "weights_2_updated = weights_2 - delta_weights_output_layer\n",
    "\n",
    "print(weights_2_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08556737 0.13387892 0.09498634 0.08556737]]\n"
     ]
    }
   ],
   "source": [
    "delta_thetas_output_layer = error_gradient_output_layer * lr\n",
    "\n",
    "thetas_2_updated = thetas_2 - delta_thetas_output_layer\n",
    "\n",
    "print(thetas_2_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate error gradients in order to be ready for hidden layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44901899  0.17211113  0.22872531]]\n"
     ]
    }
   ],
   "source": [
    "aggregated_error_gradient_layer_2 = error_gradient_output_layer.dot(weights_2.T * derivative_tanh(y_2).numpy())\n",
    "print(aggregated_error_gradient_layer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 _Weights_ between hidden layer 1 and hidden layer 2 as well as _Thetas_ in the hidden layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.98691951 -0.80501381  1.29333694]\n",
      " [ 1.32074994 -1.00795355 -0.81056979]\n",
      " [-0.81308049  1.30501381 -0.99333694]]\n"
     ]
    }
   ],
   "source": [
    "delta_weights_1 = aggregated_error_gradient_layer_2.T.dot(y_1).T * lr\n",
    "\n",
    "weights_1_updated = weights_1 - delta_weights_1\n",
    "\n",
    "print(weights_1_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1449019  0.08278889 0.07712747]]\n"
     ]
    }
   ],
   "source": [
    "delta_thetas_1 = aggregated_error_gradient_layer_2 * lr\n",
    "\n",
    "thetas_1_updated = thetas_1 - delta_thetas_1\n",
    "\n",
    "print(thetas_1_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate error gradients in order to be ready for hidden layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55701915 -0.73832976  0.32417294]]\n"
     ]
    }
   ],
   "source": [
    "aggregated_error_gradient_layer_1 = aggregated_error_gradient_layer_2.dot(weights_1.T * derivative_tanh(y_1))\n",
    "print(aggregated_error_gradient_layer_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 _Weights_ between input layer and hidden layer 1 as well as _Thetas_ in the hidden layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.94986828  0.06644968 -0.02917556]\n",
      " [-0.0668423   1.08859957 -1.03890075]\n",
      " [-1.03899134  0.05168308 -0.02269211]\n",
      " [-0.04456153 -0.94093362  0.97406616]]\n"
     ]
    }
   ],
   "source": [
    "delta_weights_0 = aggregated_error_gradient_layer_1.T.dot(X_train).T * lr\n",
    "\n",
    "weights_0_updated = weights_0 - delta_weights_0\n",
    "\n",
    "print(weights_0_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.04429809 0.17383298 0.06758271]]\n"
     ]
    }
   ],
   "source": [
    "delta_thetas_0 = aggregated_error_gradient_layer_1 * lr\n",
    "\n",
    "thetas_0_updated = thetas_0 - delta_thetas_0\n",
    "\n",
    "print(thetas_0_updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a model in Keras to verify the calculations above <span style=\"color:red\">(to be completed)</span>\n",
    "\n",
    "Note here that in order to play a fair game when validating the new weights and thetas, make sure you use the `SGD` optimizer when compiling your ANN mode:\n",
    "\n",
    "```\n",
    "model.compile(SGD(learning_rate=0.1, momentum=0.0, nesterov=False), ......)\n",
    "```\n",
    "\n",
    "Also, every time before you perform a new round of training of the ANN model, **you need to restart the kernel** such that the model always starts learning from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(3, input_shape=(4,), activation='tanh'))\n",
    "model.add(Dense(3, activation='tanh'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(SGD(learning_rate=0.1, momentum=0.0, nesterov=False),\n",
    "             loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all weights and thetas with the ones used in Section 1 above\n",
    "model.layers[0].set_weights([weights_0, thetas_0])\n",
    "model.layers[1].set_weights([weights_1, thetas_1])\n",
    "model.layers[2].set_weights([weights_2, thetas_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 0s - loss: 0.4137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17015babe08>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the ANN with the single data instance used in Section 1 above\n",
    "\n",
    "model.fit(X_train, y_train, epochs=1, verbose=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the new weights and thetas\n",
    "\n",
    "weights_2_updated_model = model.get_weights()[4]\n",
    "thetas_2_updated_model = model.get_weights()[5]\n",
    "\n",
    "weights_1_updated_model = model.get_weights()[2]\n",
    "thetas_1_updated_model = model.get_weights()[3]\n",
    "\n",
    "weights_0_updated_model = model.get_weights()[0]\n",
    "thetas_0_updated_model = model.get_weights()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validations and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The following is what is expected:\n",
    "\n",
    "```\n",
    "Checking weights_2 (weights between hidden layer 2 and output layer):\n",
    "\n",
    "[[-1.0081774   1.1191956  -0.0028407  -1.0081774 ]\n",
    " [ 0.01083027 -1.0254228   1.1037623   0.01083027]\n",
    " [ 1.0945123   0.01288169 -1.0019063   1.0945123 ]]\n",
    "[[-1.00817742  1.11919554 -0.0028407  -1.00817742]\n",
    " [ 0.01083027 -1.0254228   1.10376226  0.01083027]\n",
    " [ 1.09451232  0.01288169 -1.00190633  1.09451232]]\n",
    "They match!\n",
    "\n",
    "----------\n",
    "\n",
    "Checking thetas_2 (thetas in the output layer):\n",
    "\n",
    "[0.08556737 0.13387892 0.09498635 0.08556737]\n",
    "[[0.08556737 0.13387892 0.09498634 0.08556737]]\n",
    "They match!\n",
    "\n",
    "----------```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking weights_2 (weights between hidden layer 2 and output layer):\n",
      "\n",
      "[[-1.0081774   1.1191956  -0.0028407  -1.0081774 ]\n",
      " [ 0.01083027 -1.0254228   1.1037623   0.01083027]\n",
      " [ 1.0945123   0.01288169 -1.0019063   1.0945123 ]]\n",
      "[[-1.00817742  1.11919554 -0.0028407  -1.00817742]\n",
      " [ 0.01083027 -1.0254228   1.10376226  0.01083027]\n",
      " [ 1.09451232  0.01288169 -1.00190633  1.09451232]]\n",
      "They match!\n",
      "\n",
      "----------\n",
      "\n",
      "Checking thetas_2 (thetas in the output layer):\n",
      "\n",
      "[0.08556737 0.13387892 0.09498635 0.08556737]\n",
      "[[0.08556737 0.13387892 0.09498634 0.08556737]]\n",
      "They match!\n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Checking weights_2 (weights between hidden layer 2 and output layer):\\n')\n",
    "print(weights_2_updated_model)\n",
    "print(weights_2_updated)\n",
    "if (np.isclose(weights_2_updated_model, weights_2_updated).all()):\n",
    "    print('They match!')\n",
    "else:\n",
    "    print('They don\\'t match.')\n",
    "print('\\n----------\\n')\n",
    "\n",
    "print('Checking thetas_2 (thetas in the output layer):\\n')\n",
    "print(thetas_2_updated_model)\n",
    "print(thetas_2_updated)\n",
    "if (np.isclose(thetas_2_updated_model, thetas_2_updated).all()):\n",
    "    print('They match!')\n",
    "else:\n",
    "    print('They don\\'t match.')\n",
    "print('\\n----------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The following is what is expected:\n",
    "```\n",
    "Checking weights_1 (weights between hidden layer 1 and hidden layer 2):\n",
    "\n",
    "[[-0.9869195  -0.80501384  1.2933369 ]\n",
    " [ 1.3207499  -1.0079535  -0.8105698 ]\n",
    " [-0.8130805   1.3050138  -0.9933369 ]]\n",
    "[[-0.98691951 -0.80501381  1.29333694]\n",
    " [ 1.32074994 -1.00795355 -0.81056979]\n",
    " [-0.81308049  1.30501381 -0.99333694]]\n",
    "They match!\n",
    "\n",
    "----------\n",
    "\n",
    "Checking thetas_1 (thetas in the hidden layer 1):\n",
    "\n",
    "[0.14490189 0.08278889 0.07712747]\n",
    "tf.Tensor([[0.1449019  0.08278889 0.07712747]], shape=(1, 3), dtype=float64)\n",
    "They match!\n",
    "\n",
    "----------```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking weights_1 (weights between hidden layer 1 and hidden layer 2):\n",
      "\n",
      "[[-0.9869195  -0.80501384  1.2933369 ]\n",
      " [ 1.3207499  -1.0079535  -0.8105698 ]\n",
      " [-0.8130805   1.3050138  -0.9933369 ]]\n",
      "[[-0.98691951 -0.80501381  1.29333694]\n",
      " [ 1.32074994 -1.00795355 -0.81056979]\n",
      " [-0.81308049  1.30501381 -0.99333694]]\n",
      "They match!\n",
      "\n",
      "----------\n",
      "\n",
      "Checking thetas_1 (thetas in the hidden layer 2):\n",
      "\n",
      "[0.14490189 0.08278889 0.07712747]\n",
      "[[0.1449019  0.08278889 0.07712747]]\n",
      "They match!\n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Checking weights_1 (weights between hidden layer 1 and hidden layer 2):\\n')\n",
    "print(weights_1_updated_model)\n",
    "print(weights_1_updated)\n",
    "if (np.isclose(weights_1_updated_model, weights_1_updated).all()):\n",
    "    print('They match!')\n",
    "else:\n",
    "    print('They don\\'t match.')\n",
    "print('\\n----------\\n')\n",
    "\n",
    "print('Checking thetas_1 (thetas in the hidden layer 2):\\n')\n",
    "print(thetas_1_updated_model)\n",
    "print(thetas_1_updated)\n",
    "if (np.isclose(thetas_1_updated_model, thetas_1_updated).all()):\n",
    "    print('They match!')\n",
    "else:\n",
    "    print('They don\\'t match.')\n",
    "print('\\n----------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The following is what is expected:\n",
    "```\n",
    "Checking weights_0 (weights between input layer and hidden layer 1):\n",
    "\n",
    "[[ 0.94986826  0.06644966 -0.02917555]\n",
    " [-0.0668423   1.0885996  -1.0389007 ]\n",
    " [-1.0389913   0.05168307 -0.0226921 ]\n",
    " [-0.04456153 -0.94093364  0.9740662 ]]\n",
    "[[ 0.94986828  0.06644968 -0.02917556]\n",
    " [-0.0668423   1.08859957 -1.03890075]\n",
    " [-1.03899134  0.05168308 -0.02269211]\n",
    " [-0.04456153 -0.94093362  0.97406616]]\n",
    "They match!\n",
    "\n",
    "----------\n",
    "\n",
    "Checking thetas_0 (thetas in the input layer):\n",
    "\n",
    "[0.04429809 0.17383295 0.06758272]\n",
    "tf.Tensor([[0.04429809 0.17383298 0.06758271]], shape=(1, 3), dtype=float64)\n",
    "They match!\n",
    "\n",
    "----------```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking weights_0 (weights between input layer and hidden layer 1):\n",
      "\n",
      "[[ 0.94986826  0.06644966 -0.02917555]\n",
      " [-0.0668423   1.0885996  -1.0389007 ]\n",
      " [-1.0389913   0.05168307 -0.0226921 ]\n",
      " [-0.04456153 -0.94093364  0.9740662 ]]\n",
      "[[ 0.94986828  0.06644968 -0.02917556]\n",
      " [-0.0668423   1.08859957 -1.03890075]\n",
      " [-1.03899134  0.05168308 -0.02269211]\n",
      " [-0.04456153 -0.94093362  0.97406616]]\n",
      "They match!\n",
      "\n",
      "----------\n",
      "\n",
      "Checking thetas_0 (thetas in the hidden layer 1):\n",
      "\n",
      "[0.04429809 0.17383295 0.06758272]\n",
      "[[0.04429809 0.17383298 0.06758271]]\n",
      "They match!\n",
      "\n",
      "----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Checking weights_0 (weights between input layer and hidden layer 1):\\n')\n",
    "print(weights_0_updated_model)\n",
    "print(weights_0_updated)\n",
    "if (np.isclose(weights_0_updated_model, weights_0_updated).all()):\n",
    "    print('They match!')\n",
    "else:\n",
    "    print('They don\\'t match.')\n",
    "print('\\n----------\\n')\n",
    "\n",
    "print('Checking thetas_0 (thetas in the hidden layer 1):\\n')\n",
    "print(thetas_0_updated_model)\n",
    "print(thetas_0_updated)\n",
    "if (np.isclose(thetas_0_updated_model, thetas_0_updated).all()):\n",
    "    print('They match!')\n",
    "else:\n",
    "    print('They don\\'t match.')\n",
    "print('\\n----------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
